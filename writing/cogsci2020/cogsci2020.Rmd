---
title: "Child language input does not reflect word frequency: Typical and atypical feature description across development"
bibliography: purple-carrots.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
    Language provides children a powerful source of information about the world. However, language does not perfectly reflect the world: the most typical features of natural kinds may often go unremarked. For instance, adults rarely describe the color of an orange carrot, as world knowledge makes this description redundant. Given children’s nascent world knowledge, does parents’ speech to children follow this pattern? From longitudinal corpus data of parent-child communication (Goldin-Meadow et al., 2014) between 14–58 months, we extracted usage data for 637 high-frequency concrete nouns and co-occurring adjectives. Independent raters coded the typicality of 1,901 unique adjective–noun pairs on a 7-point Likert scale. If language statistics reflect world statistics, description should be dominated by the typical (strong negative skew); instead, across all ages, we see descriptors concentrated in the atypical range (positive skewness = 0.65). However, parents were reliably more likely to use typical descriptors when talking to younger compared with older children. Overall, child language input reflects notable more than typical features, but increased description of typical features early in development may provide a foothold for young learners.

keywords: >
    language input, language acquisition, child-directed speech
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo = F, warning = F, cache = T, 
                      message = F, sanitize = T)
# Note: to build, 
options(digits=2)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(here)
library(tidyverse)
library(ggridges)
library(scales)
library(tidyboot)
library(xtable)
library(papaja)
library(lme4)
library(lmerTest)
library(ggthemes)
library(broom.mixed)

theme_set(theme_few())
```

Children learn a tremendous amount about the structure of the world around them in just a few short years, from the rules that govern the movement of physical objects to the hierarchical structure of natural categories and even relational structures among social and cultural groups [@baillargeon1994; @rogers2004; @legare2016]. Where does the information for this rapid acquisition come from? Undoubtedly, a sizeable component comes from direct experience observing and interacting with the world [@sloutsky2004; @stahl2015]. But another important source of information comes from the language people use to talk about the world [@landauer1997; @rhodes2012]. For many of the things children learn about--like the roundness of the earth--the information must come from language [@harris2006]. How similar is the information available from children's direct experience to the information available in the language children hear? 

Two lines of work suggest that they may be surprisingly similar. One compelling area of work is the comparison of semantic structures learned by congentinally blind children to those of their sighted peers. In several domains that would at first blush rely heavily on visual information, such as color terms or verbs of visual perception (e.g. *look*, *see*), blind children's semantic similarity judgments are quite similar to those of sighted children [@landau2009]. Further, blind adults' judgments of visual perception verbs are sensitive to highly detailed information like variation in intensity (e.g. blaze vs. glow), just like sighted adults [@bedny2019]. A second line of evidence supporting the similarity of information in perception and language is the broad success of statistical models trained on language alone in approximating human judgments across a variety of domains [@landauer1997; @mikolov2013; @devlin2018]. Even more compellingly, models trained on both linguistic usage and perceptual features for some words can infer the perceptual features of linguistically related words entirely from the covariation of language and perception [@johns2012]. 

Still, there is reason to believe that some semantic features may be harder to learn from language than these data suggest. While the co-occurrence structure of language may provide strong clues that carrots are like tomatoes, carrots are vegetables, and carrots are eaten for dinner, they may provide very little evidence that carrots are orange [@willits2008]. This is because people rarely use language merely to provide running comentary on the world around them. Instead, we use language to talk about things we find interesting--things that are surprising, exciting, or otherwise divergent from our conversational partners' expectations [@grice1975]. 

Thus, speakers are informative in relation to common knowledge with their conversational partner and available information in the environment, not redundant with these other sources of knowledge. Communication tasks in the lab largely bolster this claim, finding that people avoid being over- or under-informative when they speak (Mangold & Pobel, 1988; MORE CITES) and interpret others' speech as if they are doing the same (CITES). In particular, speakers are informative in relation to both the context of objects in the environment and the typical features of those objects (Mitchell et al., 2013; Westerbeek et al., 2015; Rubio-Fernandez, 2016). While speakers overwhelmingly refer to an object that is typical of its category with a bare noun—e.g., calling a yellow banana "a banana"—they often describe more about the object's features when they are atypical, rarely referring to a green or blue banana without specifying its color. Listeners are similarly affected by color description, expecting a color adjective to be used when an object's color is atypical [@sedivy2003].

For things like carrots--that children learn about both from perception and from language--this issue may be resolved by integrating both sources of information: Likely almost all of the carrots they see are orange even if no one comments on them. But for things for which they lack perceptual access--zoo animals, other social groups, and so on--the structure of language alone may lead them to have distorted categories. This may be especially problematic because children's understanding of the pragmatics of adjective use change significantly over development [@horowitz2016]. 

Should we expect children's categories to be biased in this way, with typical features underrepresented and atypical features overrepresented? We examine the typicality of adjectives in a large, diverse corpus of parent-child ineractions recorded in children's homes to ask whether parents talking to their children--just like adults speaking and writing--tend to use adjectives predominantly to mark atypical features. We find that they do: parents and children overwhelmingly choose to mention atypical rather than typical features. However, we also find that parents use adjectives differently over the course of children's development, noting typical features more often to younger children. We then ask whether the co-occurrence structure of language nonetheless captures typicality information, and find that relatively little of it is represented. Thus, children must either have distorted representations of categories learned only through language or else learn typicality through other means.



**notes for intro word2vec more**


Information in language goes beyond what can be learned from any one utterance. Though one might never hear the words /kumquat/ and /grapefruit/ in the same sentence, their common contexts—other words like /eat/, /rind/, /tree/, /tart/, and /seeds/—are clues that they have some similarities. Models of distributional semantics capitalize on these patterns by representing words using their contexts, and judging two words to be similar if they are surrounded by similar sets of words. Though we found that children get more information about atypical than typical features of objects on the utterance level, perhaps patterns of language use across many utterances could be used to extract typical feature information. 




<!-- While such effects have been demonstrated in the lab, the extent to which these language pressures structure naturalistic language remains unclear. A recent analysis of co-occuring adjective-noun pairs in the Wikipedia corpus asked human raters to judge the whether a given adjective was 'always true', 'sometimes true', or 'never true/unrelated' to a co-occuring noun (Willits,// *personal communication * // ? ). The analysis revealed that language usage is dominated by descriptions that are only 'sometimes true' of the described category, and thus suggests that naturalistic language usage reflects pressures to comment on the atypical (Willits,// *personal communication * // ? ). -->

<!-- If the information in language statistics is indeed not a reflection of world statistics, young children may be faced with a difficult learning problem. Language provides a crucial information source for young children to learn about the complex world around them [@landauer1997; @rhodes2012]. Without the relevant world knowledge, young children learning from language input that is biased to overrepresent atypical features could develop inaccurate or distorted understandings of the categories being described.  -->



<!-- It could be that the language input to young children is far different from language on Wikipedia, such that children are learning from language that more directly reflects information about the world around them.  Child-directed speech (CDS) differs from typical adult-directed speech along a number of structural dimensions, having simpler syntax and more reduplications (Snow, 1972). The current study asks whether CDS is biased to highlight atypical features, as in adult speech (WILLITS, personal communication), or to more veridically reflect typical information about the world. -->

<!-- We first asked how information is distributed in CDS, and how similar it is to ADS. In a longitudinal corpus of parent-child communication, we collected usage data for adjective-noun pairs that co-occur within the same utterance. Human raters on Amazon Mechanical Turk then judged typicality (e.g., "How common is it for a banana to be a yellow banana?") on a 7-point scale. Using these data, we first demonstrate that child-directed speech (even to children as young as 14 months) is dominated by descriptions of lower-typicality features (e.g., "green banana"), rather than potentially redundant highly-typical features (e.g., "yellow banana").  -->

<!-- CDS also changes across development, with parents modulating the way they talk as a function of child's age (Snow, 1972). It is plausible that caregivers, especially of young children, would provide more description of typical features, whether simply to communicate effectively with a child who has less world knowledge or to explicitly teach their child about the world. This would suggest children with the least developed world knowledge actually get more information about aspects of the world that are typically true. -->

<!-- To address this possibility, we next asked if the kind of information in CDS changes as children develop. We find evidence that description changes across development, with young children hearing relatively more talk about highly-typical features compared with older children. -->

<!-- We lastly discuss what these data may imply for the developing learner. Children rapidly develop rich conceptual repetoires, yet the language children hear seems biased to overrepresent certain features. We consider a few potential cues that could allow children to extract better typicality information from language, such as tracking second-order co-occurence or syntactic cues.  -->


# Adjective typicality

In order to determine whether parents use adjectives mostly to mark atypical features of categories, we analyzed caregiver speech from a large corpus of parent-child interactions recorded in the home. We extracted a subset of adjective-noun combinations that co-occured, and asked a sample of Amazon Mechanical Turkers to judge how typical the property described by each adjective was for the noun it modified. We then examined both the broad features of this typicality distribution and how it changed over development.

## Corpus

We used data from the Language Development Project-- a large-scale, longitudinal corpus of parent-child interactions recorded in children's homes. Families were recruited to be representative of the Chicagoland area in both socio-economic and racial composition (Goldin-Meadow et al., 2014). Recordings were taken in the home every 4-months from when the child was 14-months-old until they were 58-months-old, resulting in 12 timepoints. Each recording was of a 90-minute session in which parents and children were free to behave as they liked and interact as much or as little as they liked.

Our sample consisted of 64 typically-developing children and their caregivers, with data from at least 4 timepoints (*mean* = 11.3 timepoints). Together, this resulted in a total of 641,402 distinct parent utterances.  

## Stimulus Selection

From these utterances, we extracted all of the nouns (using part of speech tags--**MANUAL? AUTOMATIC**) resulting in a set of 8,150 total nouns. Because of our interest in change over development, we considered only nouns that appeared at least once every 3 sessions (i.e. per developmental year). This yielded a set of some 1,829 potential target nouns used over 198,014 distinct utterances.

We selected from the corpus all 35,761 distinct utterances containing any of these nouns and any word tagged as an adjective. We considered for analysis all adjective-noun pairs that occurred in any utterance (i.e. utterances with one noun and three adjectives were coded as three pairs) for a total of 18,050 distinct pairs. This set contained a number of high-frequency idiomatic pairs whose typicality was difficult to classify (e.g., "good"--"job"; "little"--"bit"). To resolve this issue, we used human judgments of words' concreteness to identify and exclude candidate idioms [@brysbaert2014]. We retained for analysis only pairs where both the adjective and noun were in the top 25% of the concreteness ratings (e.g., "dirty" -- "dish"; "green" -- "fish") restricting our set to 2,477.  Finally, all pairs were given to 7 human coders to judge whether the pair was "incoherent or unrelated" and we thus excluded a final 576 pairs from the sample (e.g., incoherent pairs such as "flat" -- "honey").

Thus, our final sample included 1,901 unique adjective-noun pairs drawn from 3,749 distinct utterances. The pairs were combinations of 637 distinct concrete nouns and 111 distinct concrete adjectives. We compiled these pairs and collected human judgments on Amazon Mechanical Turk for each pair, as described below. Table \ref{tab:utt_table} contains example utterances from the final set and typicality judgments from our human raters.


```{r utt_table, results="asis", tab.env = "table"}
tab <- tibble(utterance = c("especially with wooden shoes",
                            "some of our green bananas too", 
                            "are bananas yellow?"),
              pair = c("wooden-shoe", "green-banana", "yellow-banana"),
              `rating 1` = c(2, 4, 5),
              `rating 2` = c(2, 3, 5),
              `rating 3` = c(3, 5, 7),
              `rating 4` = c(2, 3, 6),
              `mean typicality` = c(2.75, 3.75, 5.75)) %>% 
  xtable(display = c("s", "s", "s", "d", "d", "d", "d", "f"),
         caption = "Sample typicality ratings from 4 human coders for three adjective-noun pairs drawn from the corpus.",
         label = "tab:utt_table")

print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table*",
      include.rownames = FALSE)
```


## Participants

Each participant rated 20 pairs, and each pair was rated by four participants; we used [Dallinger](http://docs.dallinger.io/en/latest/), a tool for automating complex recruitment on Amazon Mechanical Turk, to balance recruitment. Overall, we recruited 444 participants to rate 2200 adjective–noun pairs. After exclusions using an attention check **what was the check?**, we retained 8580 judgments, with each adjective–noun pair retaining at least two judgments.

## Design and Procedure

To evaluate the typicality of the adjective–noun pairs that appeared in parents' speech, we asked participants on Amazon Mechanical Turk to rate each pair. Participants were presented with a question of the form “How common is it for a cow to be a brown cow?” and asked to provide a rating on a seven-point scale: (1) never, (2) rarely, (3) sometimes, (4) about half the time, (5) often, (6) almost always, (7) always. These ratings were combined with usage data from our corpus analysis to let us determine the extent to which parents use language to describe typical and atypical features.

```{r read_tokens}
token_data <- read_csv(here("data/clean_token_data.csv"))

coca_data <- read_csv(here("data/adult_tokens_withn.csv")) %>%
  filter(!is.na(n))

kid_production <- read_csv(here("data/kid_production.csv")) %>%
  mutate(age = 10 + 4 * session) %>%
  select(adj, noun, n, age) %>%
  distinct() %>%
  left_join(select(token_data, adj, noun, x1:x5) %>% distinct(), by = c("adj", "noun")) %>%
  pivot_longer(x1:x5, names_to = "rater", values_to = "score")

kid_total <- kid_production %>%
  distinct(adj, noun) %>%
  count() %>%
  pull()

kid_frac <- kid_production %>%
  group_by(adj, noun) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  filter(!is.na(score)) %>%
  ungroup() %>%
  count() %>%
  pull()

type_data_by_session <- token_data %>%
  group_by(adj, noun, session, age, noun_conc, noun_freq, 
           article, x1, x2, x3, x4, x5, mean_typ) %>%
  summarize(n=n())
```


```{r distribution_plot, fig.height = 6, fig.width=3.6, fig.align = "center", num.cols.cap=1, fig.cap = "Denisity plots showing the usage amount at each timepoint based on the typicality of the adj-noun pair."}
# full_turk_counts <- read_csv(here("data/judgments_session.csv"))
# 
# full_turk_counts_plot <- full_turk_counts
# 
# full_turk_counts_plot$session2 <- cut(full_turk_counts_plot$session, 
#                    breaks=c(0, 2, 4, 6, 8, 10, 12),
#                    labels = c(14, 22, 30, 38, 42, 50))
#                    # breaks=c(0, 3, 6, 9, 12, 10, 12))

# full_expanded <- full_turk_counts[rep(row.names(full_turk_counts), full_turk_counts$n), 1:17]

token_data %>%
  mutate(typicality=mean_typ) %>%
  mutate(age = (4*session + 10)) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))
```

```{r models}
model_judgments <- read_csv(here("data/all_judgments.csv"))

tidy_turk_counts <- type_data_by_session %>%
    # select(-noun_conc, -noun_freq) %>%
    pivot_longer(cols = x1:x5, names_to = "rater", values_to = "score") %>%
  ungroup() %>%
    mutate(centered_score = score - 4,
           article = case_when(is.na(article) ~ "",
                               article %in% c("a","an") ~ "a/an",
                               T ~ "the")) %>%
  left_join(model_judgments, by = c("adj","noun"))

tidy_turk_counts_for_plots <- tidy_turk_counts %>%
  distinct(noun,adj,turker_judgment,ldp_similarity,wiki_similarity)

mean_type <- lmer(centered_score ~ 1 + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_type_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                     data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()

mean_token_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                                  weights = n, data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()


mean_token <- lmer(centered_score ~ 1 + (1|noun), weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


token_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


type_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


mean_coca_token <- coca_data %>%
  filter(type == "spok") %>%
  pivot_longer(cols = c(x1:x5), names_to = "rater", values_to = "score") %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_kid_token <- kid_production %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")


```

## Results

To estimate the relative frequency with which caregivers refer to typical vs. atypical referents, we used Turker's judgments about the atypicality of each adjective-noun pair in the data. In our analyses, we token-weighted these judgments--giving higher weight to pairs that occurred more frequently in children's inputs. However, results are qualitatively identical and all signiifcant effects remain significant without these re-weightings.

If caregivers speak informatively to convey what is atypical or surprising in relation to their own sophisticated world knowledge, we should see that caregiver description is dominated by modifiers that are sometimes true of the noun they modify. If instead child-directed speech privledges redundant information perhaps to align to young children's limited world knowledge, caregiver description should yield a distinct distribution dominated by highly typical modifiers. As can be seen in Figure \ref{fig:distribution_plot} parents' description largely focuses on features that are only sometimes true of the concept.

To confirm this effect statistically, we first centered the ratings (i.e. "about half" was coded 0), and then predicted the rating on each trial with a mixed effect model with only an intercept and a random effect of noun (\texttt{typicality $\sim$ 1 + (1|noun)}). The intercept was reliably negative, indicating that adjective tends to refer to atypical features of objects ($\beta =$ `r mean_token %>% pull(estimate)`, 
$t =$ `r mean_token %>% pull(statistic)`, $p$ `r mean_token %>% pull(p.value) %>% printp()`). We then re-estimated these models seperately for each age in the corpus, and found a reliablly negative intercept for every age group (smallest effect $\beta_{14} =$ `r mean_token_byage %>% slice(1) %>% pull(estimate)`, 
$t =$ `r mean_token_byage %>% slice(1) %>% pull(statistic)`, $p =$ `r mean_token_byage %>% slice(1) %>% pull(p.value) %>% printp()`).  These data suggest that even when talking with very young children, caregiver speech is structured according to communicative pressures observed in the lab. 

<!-- Examining usage data as a function of typicality (see Figure \ref{fig:distribution_plot}), we see evidence of a positive skew (0.65). Data from every time point from 14-58 months seems to show a similar pattern (skews 0.23 - 0.82). These skews provide further evidence that the the bulk of caregiver language reflects lower-typicality adjective-noun pairs. -->

For comparison, we performed the same analyses but with typicality judgments weighted not by the frquency of each adjective-noun pair's occurrence in the Language Development Project, but instead by their frequency of occurrence in the Corpus of Contemporary American English [COCA; @davies2008]. While this estimate of adult usage is imperfect--as the adjective-nouns pairs produced by parents in our corpus may not be a representative sample of adjectives and nouns spoken by the adults in COCA, it provides a first approximation to adult usage. When we fit the same mixed-effects model to the data, we found that the intercept was reliably negative, indicating that adult-directed speech is likely also biased toward description of atypical features ($\beta =$ `r mean_coca_token %>% pull(estimate)`, 
$t =$ `r mean_coca_token %>% pull(statistic)`, $p$ `r mean_coca_token %>% pull(p.value) %>% printp()`)

Returning to caregiver speech, while descriptions at every age tended to point out atypical features as in adult-direct speech, this effect changed in strength over development. An age effect added to the previous model was reliably negative, indicating that parents of older children are relatively more likely to focus on atypical features  ($\beta =$ `r type_weight %>% filter(term == "log(age)") %>% pull(estimate)`, 
$t =$ `r type_weight %>% filter(term == "log(age)") %>% pull(statistic)`, $p =$ `r type_weight %>% filter(term == "log(age)") %>% pull(p.value) %>% printp()`). This effect also remained significant when pairs were weighted according the their frequency ($\beta =$ `r token_weight %>% filter(term == "log(age)") %>% pull(estimate)`, 
$t =$ `r token_weight %>% filter(term == "log(age)") %>% pull(statistic)`, $p =$ `r token_weight %>% filter(term == "log(age)") %>% pull(p.value) %>% printp()`).


```{r compute_prototypicals}
prototypical_ratings <- type_data_by_session %>%
  mutate(typical = mean_typ >= 5)

#look for prototypicals
#  defined as anything 5 or higher, "somewhat typical" to "extremely  typical"
prototypicals <- prototypical_ratings %>%
  group_by(age, typical) %>%
  summarize(weighted_sum = sum(n), sum = n()) %>%
  group_by(age, typical) %>%
  summarize(sum = sum(sum), weighted_sum = sum(weighted_sum)) %>%
  pivot_longer(cols = c(sum, weighted_sum), names_to = "measure", values_to = "sum") %>%
  group_by(age, measure) %>%
  mutate(prop = sum / sum(sum)) %>%
  filter(typical)


typical_type_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")

typical_token_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
                      weights = n,
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")
```

Though there is striking consistency of description in caregiver speech across development, we found parents of older children were more likely to refer to atypical features. In line with our hypotheses, it seems that caregivers are more likely to provide description of typical features for their young children, compared with older children. To confirm this intuition, we defined adjectives as highly-typical if Turkers judged them to be 'often', 'almost always', or 'always' true. We predicted whether each judgment was highly typical from a mixed-effects logistic regression with a fixed effect of age (log-scaled) and a random effect of noun. Age was a highly-reliable predictor ($\beta =$ `r typical_type_lmer %>% filter(term == "log(age)") %>% pull(estimate)`, 
$t =$ `r typical_type_lmer %>% filter(term == "log(age)") %>% pull(statistic)`, $p =$ `r typical_type_lmer %>% filter(term == "log(age)") %>% pull(p.value) %>% printp()`). While children at all ages hear more talk about what is atypically true (Figure  \ref{fig:distribution_plot}), younger children hear relatively more talk about what is typically true than older children do (Figure \ref{fig:prototypical_plot}).


```{r prototypical_plot, fig.width=3.6, fig.align = "center", num.cols.cap=1, fig.cap = "Proportion of caregiver description that is about typically-true features, as a function of age."}

prototypicals %>% 
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n typical of modified noun") +
  xlab("Child's Age (months)") +
  scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none",
        aspect.ratio = 1/1.62) 
```



### Child Speech.

Given the striking consistency in adult-to-adult speech and caregiver speech across ages, we next briefly consider what kind of information is contained in children's speech. By analyzing children's own utterances, we can determine when children come to use description in a way that looks like caregiver speech. Are children mirroring adult-like uses of description even from a young age, or are they choosing to describe more typical features of the world?

The Language Development Corpus contains 442,048 child utterances. Using the set of adjective-noun pairs for which we have judgments from our analysis of caregiver speech, we repeat our analysis on usage data for a set of `r kid_total` distinct adjective-noun pairs, `r kid_frac` of which appeared in children's productions. While preliminary, a mixed effects model predicting typicality had a highly-reliable negative intercept ($\beta =$ `r mean_kid_token %>% filter(term == "(Intercept)") %>% pull(estimate)`, $t =$ `r mean_kid_token %>% filter(term == "(Intercept)") %>% pull(statistic)`, $p =$ `r mean_kid_token %>% filter(term == "(Intercept)") %>% pull(p.value) %>% printp()`),  but adding an age term did not improve model fit. Thus, children's speech is also biased towards atypical descriptions, and this bias does not change reliably over the first 5 years.

<!-- Mirroring caregiver and adult-adult speech, children's productions show a positive skew (0.61, comapre with skewness = 0.65 seen in the adults), such that the bulk of language reflects adjective-noun pairs rated < 4 on typicality (i.e. 'sometimes', 'rarely' or 'never'). -->


## Discussion


In sum, language is used to discuss atypical, rather than typical, features of the world. Description in caregiver speech seems to largely mirror the usage patterns that we observed in adult-adult speech, suggesting that these patterns arise from general communicative pressures. Indeed, even children's own productions show a similar usage pattern, with more description of atypical features of the world as early as we can measure.

It should be noted that children's utterances come from naturalistic conversations with caregivers, and their pattern of description is likely highly dependent on their caregiver's utterances. That is, if a caregiver chooses to describe the *purpleness* of a cat in book, the child may well respond by asking about that same feature. Thus, it is possible that some of the children's use of atypical description is prompted by a parent-led discourse. Future analyses would need to better disentangle the extent to which children's productions are imitative of caregivers. 

While children's own descriptions largely mirror adults', the descriptions children hear change over development, becoming increasingly focused on atypical features. The higher prevalance of typical descriptors in early development may help young learners; however, even at the earliest point we measured, the bulk of language input describes atypical features. 

Across adult, parent, and child language corpora, we find robust evidence that language use systematically overerpresents atypical features. This usage aligns with the idea that language is used informatively in relation to background knowledge about the world. It may pose a problem, however, for young language learners with still-developing world knowledge. If language does not transparently convey the typical features of objects, and instead (perhaps misleadingly) notes the atypical ones, how might children come to learn what objects are typically like? One possibility is that information about typical features is captured in regularities across many utterances. If this is true, language may still be an important source of information about typicality as children may be able to extract more accurate typicality information by tracking second-order co-occurence.


# Extracting Structure from Language

Much information can be gleaned from language that does not seem available at first glance. From language alone, simple distributional learning models can recover enough information to perform comparably to non-native college applicants on the Test of English as a Foreign Language [@landauer1997]. Recently, @lewis2019 demonstrated that even nuanced feature information may be learnable through distributional semantics alone, without any complex inferential machinery. We take a similar approach to ask whether a distributional semantics model trained on the language children hear can capture typical feature information.

```{r, eval = FALSE}
split_half <- tidy_turk_counts %>%
  mutate(rater = as.numeric(gsub("x", "", rater))) %>%
  filter(rater %in% c(1, 2)) %>%
  group_by(rater, noun, adj) %>%
  summarise(score = mean(score, na.rm = T)) %>%
  pivot_wider(names_from = rater, values_from = score)

half_model <- lmer(`1` ~ `2` + (1|adj) + (1|noun), data = split_half)

split_half %>%
  ungroup() %>%
  mutate(prediction = predict(half_model)) %>%
  summarise(cor = cor(`1`, prediction))

data <- tidy_turk_counts_for_plots %>% 
       left_join(tidy_turk_counts %>% filter(rater == "x1"))

cor(data$turker_judgment, data$wiki_similarity, use = "pairwise")

wiki_model <- lm(score ~ wiki_similarity ,
     data = data) 

tidy_turk_counts_for_plots %>%
  filter(!is.na(wiki_similarity)) %>%
  mutate(prediction = predict(wiki_model)) %>%
  summarise(cor = cor(prediction, turker_judgment))

```

```{r word2vec-pairs, include = FALSE, eval = FALSE}
pairs <- tidy_turk_counts %>%
  group_by(noun, adj) %>%
  summarise(typicality = mean(score, na.rm = TRUE)) %>%
  group_by(noun) %>%
  mutate(max_typ = max(typicality),
         min_typ = min(typicality)) %>%
  distinct(noun, min_typ, max_typ) %>%
  filter(min_typ != max_typ, max_typ >= 5, min_typ <= 3) 

high_low_pairs <- tidy_turk_counts_for_plots %>%
  filter(noun %in% pairs$noun) %>%
  left_join(pairs, by = c("noun")) %>%
  filter(turker_judgment == min_typ | turker_judgment == max_typ) %>%
  select(adj, noun, turker_judgment) %>%
  group_by(noun) %>%
  arrange(noun, desc(turker_judgment)) %>%
  slice(1, n()) %>%
  mutate(typicality = c("high", "low"),
         typicality = factor(typicality, levels = c("low", "high"))) %>%
  left_join(tidy_turk_counts_for_plots) 

lmer(ldp_similarity ~ typicality + (1|noun) + (1|adj), data = high_low_pairs) %>%
  summary()

high_low_pairs %>%
  pivot_longer(cols = c(turker_judgment, wiki_similarity, ldp_similarity),
              names_to = "measure", values_to = "score") %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_grid(measure ~ ., scale = "free") +
  geom_point(alpha = .5) +
  geom_line(color = "light gray") +
  theme_few()


high_low_pairs %>%
  group_by(noun) %>%
  summarise(diff = first(ldp_similarity) - last(ldp_similarity)) %>%
  filter(diff < 0) %>%
  left_join(high_low_pairs %>% select(noun, adj)) %>%
  group_by(noun) %>%
  mutate(typicality = c("high", "low")) %>%
  pivot_wider(names_from = "typicality", values_from = "adj") %>%
  select(diff, noun, high, low) %>%
  View()

# filter to prenominals?

# tidy_turk_pairs <- tidy_turk_counts %>%
#     filter(noun %in% pairs$noun) %>%
#     group_by(noun, adj) %>%
#     filter(typicality <= 3 | typicality >= 5) %>%
#     group_by(noun) %>%
#     arrange(desc(typicality)) 
    #slice(c(1, n())) %>%
    #mutate(n = 1:n()) %>%
    #spread(n, adj)


```

### Word2Vec

To test this possibility, we trained word2vec, a model that predicts words using their contexts, on the same corpus of child-directed speech used in our first set of analyses. Our model is a continuous-bag-of-words word2vec model trained using the package gensim [@rehurek2010]. If the model captures information about the typical features of objects, we should see that the model's word pair similarities are correlated with the typicality ratings we elicited from human raters.  

### Results

We find that similarities in the model have near zero correlation with human adjective–noun typicality ratings (r = 0.018). This is in spite of better correlations with large sets of human similarity judgments between different kinds of word pairs (correlation with  wordsim353, 0.37; correlation with simlex, 0.15). This suggests that statistical patterns in child-directed speech are likely insufficient to encode information about the typical features of objects, despite encoding at least some information about word meaning more broadly. However, the corpus on which we trained this model was small; perhaps our model did not get enough language to draw out the patterns that would reflect the typical features of objects. To test this possibility, we asked whether word vectors trained on a much larger corpus—English Wikipedia—strongly correlate with typicality ratings. We find that while the correlation between similarities in the Wikipedia–trained model and human noun–adjective typicality ratings is stronger, it is still fairly weak at r = 0.24. Strikingly, the correlation between similarities in our model and similarities in the Wikipedia-trained models is stronger (r = 0.33) than either model's correlations with human judgments. This suggests that these models are picking up on some systematic associations between nouns and adjectives, but not typical ones. Overall, these results suggest that models of distributional semantics fail to extract typical feature information from language in which atypical features are more often described.

```{r word2vec1, fig.align='center', fig.width = 4, set.cap.width=T, num.cols.cap=1, fig.cap = "Correlation between vector similarities in word2vec (trained on the LDP corpus) and human-rated typicality judgments for our noun-adjective pairs."}
#img <- png::readPNG(here("writing/cogsci2020/figs/word2vec.png"))
tidy_turk_counts_for_plots %>%
  ggplot() +
  geom_jitter(aes(turker_judgment, ldp_similarity), width = 0.4, height = 0.4, size = 0.1) +
  ylab("Similarity in Word2Vec trained on LDP") +
  xlab("Typicality rating from human participants")


#tidy_turk_counts_for_plots %>%
 # group_by(noun,adj) %>%
  #ggplot() +
  #geom_jitter(aes(turker_judgment, wiki_similarity), width = 0.4, height = 0.4, size = 0.1) 
#grid::grid.raster(img)
```


```{r word2vec2, fig.align='center', fig.width = 4, set.cap.width=T, num.cols.cap=1, fig.cap = "Correlation between vector similarities in word2vec trained on the LDP corpus and vector similarities in word2vec trained on English Wikipedia for our noun-adjective pairs."}

tidy_turk_counts_for_plots %>%
  group_by(noun,adj) %>%
  ggplot() +
  geom_jitter(aes(ldp_similarity, wiki_similarity), width = 0.4, height = 0.4, size = 0.1) +
  ylab("Similarity in Word2Vec trained on LDP") +
  xlab("Similarity in Word2Vec trained on Wikipedia")

```

<!-- ## Age of Acquisition -->

<!-- Given our finding that young children hear relatively more description of typical features, it could be that linguistic input is more helpfully structured for the developing learner if we have a more nuanced measure linguistic experience. For example, one possibility is that caregivers are selectively overdescribing (or at least describing typical features) of concepts unfamiliar to their child. This possibility makes a clear prediction: namely, that information in CDS to children at a given age may differ as a function of whether the child knows the word being described, or not.  -->

<!-- To address this question we combined age of acquisition data from Wordbank and from adult-generated recollections (Kuperman et al., 2012).  -->



```{r kid_plot, include=FALSE, fig.height = 5.5, fig.width=3.6, fig.align = "center", num.cols.cap=1, fig.cap = "Analyzing child productions, these denisity plots show the usage amount at each timepoint based on the typicality of the adj-noun pair. Note that at our earliest timepoints (14 and 18 months), children were not reliably producing enough utterances with adjectives and nouns to be included here."}
kid_production <- read.csv("../../data/kid_production.csv")

kid_production %>%
  mutate(typicality=mean_typ) %>%
  mutate(age = session) %>%
  mutate(age = (4*session + 10)) %>%
  ggplot( aes(x=as.numeric(typicality), y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  theme_minimal() +
  scale_fill_gradient(low="cornsilk", high=muted("purple")) +
  theme(panel.grid = element_line(color="lightgrey",size=0.5),
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text = element_text(size=12),
    axis.text.x = element_text(angle=30, hjust=1),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +  
  scale_y_continuous(minor_breaks = seq(22, 58, 4), breaks = seq(22, 58, 4)) +
  geom_vline(xintercept=4)
```



```{r adult_directed, include = FALSE, fig.height=4, fig.width=3.8, fig.align = "left", num.cols.cap=1, fig.cap = "Looking at COCA data, these denisity plots show the usage amount based on the typicality of the adj-noun pair, seperated by the type of language (e.g., written)."}
adult_tokens <- read_csv(here("data/adult_tokens_withn.csv")) %>%
  mutate(type = factor(type, levels=c("acad", "fic", "mag", "news", "spok"), 
         labels=c("Academic \n Journals", "Fiction", "Magazines", 
                  "Newspapers", "Spoken \n(TV + radio)")))

adult_tokens %>%
  ggplot(aes(x=as.numeric(mean_typ), y=type, group=type, fill=type)) +
  geom_density_ridges2(scale=1.4) +
  ylab("Source of Adult Speech") +
  xlab("More Atypical                     More Typical \n Typicality of adjective-noun pairs") +
  theme_minimal() +
  theme(panel.grid = element_line(color="lightgrey",size=0.5),
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x  = element_text(size=10, angle=30, hjust=1),
    axis.text.y = element_text(size=10, angle=25),
    legend.position = "none") +
  geom_vline(xintercept = 4) +
    scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always'))+
      scale_fill_brewer(palette = "Accent") 
```


# General Discussion

Language provides children a rich source of information about the world. However, this information is not always transparently available: language’s use to comment on the atypical and surprising means it does not perfectly mirror the world. Among adult conversational partners whose world knowledge is well-aligned, this characteristic of language allows people to converse informatively and not redundantly. But between a child and caregiver whose world knowledge is asymmetric, this pressure competes with other demands: what is minimally informative to an adult may be misleading to a child. In this work, we demonstrate that this pressure structures language to create a peculiar learning environment: one in which caregivers predominantly point out the atypical features of things. 

How, then, do children learn about the typical features of things from such an environment? While younger children may gain an importnat foothold from hearing more description of typical features, they still face language dominated by atypical description. Further when we looked at more nuanced ways... , we found that models of distributional semantics capture little typical feature information.

Of course, one source of information that may simplify this problem is perceptual information from the world itself. In many cases, perceptual information may swamp information from language; children likely see enough orange carrots in the world to outweigh hearing "purple carrot.” It remains unclear, however, how children learn about categories for which they have scarcer evidence. Indeed, language information likely swamps perceptual information for many other categories, such as abstract concepts or those that cannot be learned about by direct experience. Given that the present work is limited to concrete concepts, we can only speculate about the information caregivers provide about abstract concepts. But, if abstract concepts pattern similarly to concrete objects, children are in a particularly difficult bind. Though perceptual information is undoubtedly useful in learning about the typical features of things, it remains to be explained how children learn what is typical when this perceptual information is scant, irrelevant, or incomplete.

Another possibility is that children expect language to be used informatively at a young age. Under this hypothesis, their language environment is not misleading at all. If young children expect adjectives to mark atypical features, they can use description and the lack thereof to learn more about the world around them. Lab studies find that children expect interlocutors to be informative in relation to their prior knowledge by the age of 2 (Akhtar et al., 1996) and that 5–6-year-old children are roughly informative with respect to their interlocutor’s perspective in a referential communication task (Nadig & Sedivy, 2002). More naturalistic studies find that children at the one-word stage selectively imitate words that convey new information—that is, words that are informative with respect to what is obvious or assumed in the environment—when describing events (Greenfield & Zukow, 1978). The idea that young children understand the informative purpose of language is consistent with our finding that even young children also largely choose to describe atypical features. Though this effect can be explained by simpler means such as salience or mimicry, it suggests that caregivers and children may be usefully aligned in the aspects of the world they choose to talk about.

Whether adult-directed, child-directed, or a child's own speech, language is used with remarkable consistency: people talk about the atypical. Though parents might reasonably be broadly over-informative in order to teach their children about the world, this is not the case. This presents a potential puzzle for young learners who have limited world knowledge and limited pragmatic inferential abilities. Perceptual information and nascent pragmatic abilities may help fill in the gaps, but much remains to be explored to link these explanations to actual learning. Communication pressures are pervasive forces structuring the language children hear, and future work must disentangle whether children capitalize on them or are misled by them in learning about the world.

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
