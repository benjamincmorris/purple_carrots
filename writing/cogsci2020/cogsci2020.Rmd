---
title: "Child language input does not reflect word frequency: Typical and atypical feature description across development"
bibliography: purple-carrots.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
  How do children learn the typical features of objects in the world? For many objects, this information must come from the language they hear. However, language does not veridically reflect the world: People are more likely to talk about atypical features (e.g. "purple carrot") than typical features ("e.g. orange carrot"). Does the speech that children hear from their parents also overrepresent atypical features? We estimated the typicality of features described by adjectives produced by parents in a large, longitudinal corpus of parent-child interaction. Across nearly 2000 unique adjective–noun pairs, we found that parents generally highlight atypical features of objects, but also that they are more likely to describe the typical features of objects when their children are younger. We also found that vector space models trained on linguistic input recovered very little of this typicality information. These results suggest that young children may either have warped estimates of typical features for categories they learn about through language or use other information to acquire adult-like typicality information.

keywords: >
    language input, language acquisition, child-directed speech
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo = F, warning = F, cache = T, 
                      message = F, sanitize = T)
# Note: to build, 
options(digits=2)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(here)
library(ggridges)
library(scales)
library(tidyboot)
library(xtable)
library(papaja)
library(lme4)
library(lmerTest)
library(ggthemes)
library(broom.mixed)
library(weights)
library(tidyverse)

theme_set(theme_few())
```

Children learn a tremendous amount about the structure of the world around them in just a few short years, from the rules that govern the movement of physical objects to the hierarchical structure of natural categories and even relational structures among social and cultural groups [@baillargeon1994; @rogers2004; @legare2016]. Where does the information driving this rapid acquisition come from? Undoubtedly, a sizeable component comes from direct experience observing and interacting with the world [@sloutsky2004; @stahl2015]. But another important source of information comes from the language people use to talk about the world [@landauer1997; @rhodes2012]. How similar is the information available from children's direct experience to the information available in the language children hear? 

Two lines of work suggest that they may be surprisingly similar. One compelling area of work is the comparison of semantic structures learned by congentinally blind children to those of their sighted peers. In several domains that would at first blush rely heavily on visual information, such as color terms and verbs of visual perception (e.g. *look*, *see*), blind children's semantic similarity judgments are quite similar to those of sighted children [@landau2009]. Further, blind adults' judgments of visual perception verbs are sensitive to highly detailed information like variation in intensity (e.g. blaze vs. glow), just like sighted adults [@bedny2019]. A second line of evidence supporting the similarity of information in perception and language is the broad success of statistical models trained on language alone in approximating human judgments across a variety of domains [@landauer1997; @mikolov2013]. Even more compellingly, models trained on both linguistic usage and perceptual features for some words can infer the perceptual features of linguistically related words entirely from the covariation of language and perception [@johns2012]. 
<!-- @devlin2018 -->

Still, there is reason to believe that some semantic features may be harder to learn from language than these data suggest. This is because we rarely use language merely to provide running commentary on the world around us; instead, we use language to talk about things that diverge from our expectations or those of our conversational partner [@grice1975]. People tend to avoid being over- or under-informative when they speak. In particular, when referring to objects, people are informative with respect to both the referential context and the typical features of the the object to which they are referring [@westerbeek2015; @rubio-fernandez2016]. People tend to refer to an object that is typical of its category with a bare noun (e.g., calling an orange carrot "a carrot"), but often specify when an object has an atypical feature (e.g, "a purple carrot"). Given these communicative pressures, naturalistic language statistics may provide surprisingly little evidence about what is typical [@willits2008]. 

<!-- This is because people rarely use language merely to provide running comentary on the world around them. Instead, we use language to talk about things we find interesting--things that are surprising, exciting, or otherwise divergent from our conversational partners' expectations [@grice1975].  -->

<!--When faced with an object that is typical of its kind, speakers overwhelmingly refer to that object with a bare noun- e.g., calling a banana that is yellow "a banana." But, when faced with an object that is atypical of its kind, speakers consistenly produce description as well- e.g., calling a banana that is blue "a blue banana." This allows speakers to be as informative as needed, providing no description when it shoudl be assumed (yellow banana) and providing information when it is unexpected (blue banana). These two empirical, lab-based phenomena demonstrate communicative pressures which, at scale, could mean naturalistic language is structured to contain much more description of what is atypical.-->

If parents speak to children in this minimally informative way, children may be faced with input that emphasizes atypicality in relation to world knowledge they do not yet have. For things like carrots--which children learn about both from perception and from language--this issue may be resolved by integrating both sources of information. Likely almost all of the carrots children see are orange, and hearing an atypical exemplar noted as a "purple carrot" may make little difference in their inferences about the category of carrots more broadly. But for things to which they lack perceptual access--such as rare objects, unfamiliar social groups, or inaccessible features like the roundness of the Earth--much of the information must come from language [@harris2006]. If language predominantly notes atypical features rather than typical ones, children may overrepresent atypical features as they learn the way things in the world tend to be.

On the other hand, parents may speak to children far differently from the way they speak to other adults. Parents' speech may reflect typical features of the world more veridically, or even emphasize typical features in order to teach children about the world. Parents alter their speech to children along a number of structural dimensions, using simpler syntax and more reduplications [@snow1972]. Their use of description may reflect similar alignment to children's growing knowledge.

We examine the typicality of adjectives in a large, diverse corpus of parent-child interactions recorded in children's homes to ask whether parents talking to their children--just like adults speaking and writing--tend to use adjectives predominantly to mark atypical features. We find that they do: Parents and children overwhelmingly choose to mention atypical rather than typical features. We also find that parents use adjectives differently over the course of children's development, noting typical features more often to younger children. We then ask whether the co-occurrence structure of language nonetheless captures typicality information by training vector space models on child-directed speech. We find that relatively little typical feature information is represented in these semantic spaces. 

# Adjective typicality

In order to determine whether parents use adjectives mostly to mark atypical features of categories, we analyzed caregiver speech from a large corpus of parent-child interactions recorded in the home. We extracted a subset of adjective-noun combinations that co-occured, and asked a sample of Amazon Mechanical Turkers to judge how typical the property described by each adjective was for the noun it modified. We then examined both the broad features of this typicality distribution and how it changed over development.

## Corpus

We used data from the Language Development Project, a large-scale, longitudinal corpus of parent-child interactions recorded in children's homes. Families were recruited to be representative of the Chicagoland area in both socio-economic and racial composition [@goldin-meadow2014]. Recordings were taken in the home every 4 months from when the child was 14 months old until they were 58 months old, resulting in 12 timepoints. Each recording was of a 90-minute session in which parents and children were free to behave as they liked and interact as much or as little as they liked.

Our sample consisted of 64 typically-developing children and their caregivers with data from at least 4 timepoints (*mean* = 11.3 timepoints). Together, this resulted in a total of 641,402 distinct parent utterances.  

## Stimulus Selection

From these utterances, we extracted all of the nouns (using human-coded part of speech tags) resulting in a set of 8,150 total nouns. Because of our interest in change over development, we considered only nouns that appeared at least once every 3 sessions (i.e. per developmental year). This yielded a set of some 1,829 potential target nouns used over 198,014 distinct utterances.

We selected from the corpus all 35,761 distinct utterances containing any of these nouns and any word tagged as an adjective. We considered for analysis all adjective-noun pairs that occurred in any utterance (i.e. utterances with one noun and three adjectives were coded as three pairs) for a total of 18,050 distinct pairs. This set contained a number of high-frequency idiomatic pairs whose typicality was difficult to classify (e.g., "good"--"job"; "little"--"bit"). To resolve this issue, we used human judgments of words' concreteness to identify and exclude candidate idioms [@brysbaert2014]. We retained for analysis only pairs where both the adjective and noun were in the top 25% of the concreteness ratings (e.g., "dirty" -- "dish"; "green" -- "fish") restricting our set to 2,477.  Finally, all pairs were given to 7 human coders to judge whether the pair was "incoherent or unrelated" and we thus excluded a final 576 pairs from the sample (e.g., incoherent pairs such as "flat" -- "honey").

Thus, our final sample included 1,901 unique adjective-noun pairs drawn from 3,749 distinct utterances. The pairs were combinations of 637 distinct concrete nouns and 111 distinct concrete adjectives. We compiled these pairs and collected human judgments on Amazon Mechanical Turk for each pair, as described below. Table \ref{tab:utt_table} contains example utterances from the final set and typicality judgments from our human raters.


```{r utt_table, results="asis", tab.env = "table"}
tab <- tibble(utterance = c("especially with wooden shoes.",
                            "you like red onions?", 
                            "the garbage is dirty."),
              pair = c("wooden-shoe", "red-onion", "dirty-garbage"),
              `rating 1` = c(2, 3, 7),
              `rating 2` = c(2, 5, 7),
              `rating 3` = c(3, 3, 5),
              `rating 4` = c(2, 4, 7),
              `mean typicality` = c(2.75, 3.75, 6.5)) %>% 
  xtable(display = c("s", "s", "s", "d", "d", "d", "d", "f"),
         caption = "Sample typicality ratings from 4 human coders for three adjective-noun pairs drawn from the corpus.",
         label = "tab:utt_table")

print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table*",
      include.rownames = FALSE)
```


## Participants

Each participant rated 20 pairs, and each pair was rated by four participants; we used [Dallinger](http://docs.dallinger.io/en/latest/), a tool for automating complex recruitment on Amazon Mechanical Turk, to balance recruitment. Overall, we recruited 444 participants to rate 2200 adjective–noun pairs. After exclusions using an attention check **what was the check?**, we retained 8580 judgments, with each adjective–noun pair retaining at least two judgments.

## Design and Procedure

To evaluate the typicality of the adjective–noun pairs that appeared in parents' speech, we asked participants on Amazon Mechanical Turk to rate each pair. Participants were presented with a question of the form “How common is it for a cow to be a brown cow?” and asked to provide a rating on a seven-point scale: (1) never, (2) rarely, (3) sometimes, (4) about half the time, (5) often, (6) almost always, (7) always. These ratings were combined with usage data from our corpus analysis to let us determine the extent to which parents use language to describe typical and atypical features.

```{r read_tokens}
token_data <- read_csv(here("data/clean_token_data.csv"))

coca_data <- read_csv(here("data/adult_tokens_withn.csv")) %>%
  filter(!is.na(n))

kid_production <- read_csv(here("data/kid_production.csv")) %>%
  mutate(age = 10 + 4 * session) %>%
  select(adj, noun, n, age) %>%
  distinct() %>%
  left_join(select(token_data, adj, noun, x1:x5) %>% distinct(), by = c("adj", "noun")) %>%
  pivot_longer(x1:x5, names_to = "rater", values_to = "score")

kid_total <- kid_production %>%
  distinct(adj, noun) %>%
  count() %>%
  pull()

kid_frac <- kid_production %>%
  group_by(adj, noun) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  filter(!is.na(score)) %>%
  ungroup() %>%
  count() %>%
  pull()

type_data_by_session <- token_data %>%
  group_by(adj, noun, session, age, noun_conc, noun_freq, 
           article, x1, x2, x3, x4, x5, mean_typ) %>%
  summarise(n = n())
```


```{r distribution_plot, fig.height = 6, fig.width=3.6, fig.align = "center", num.cols.cap=1, fig.cap = "Denisity plots showing the usage amount at each timepoint based on the typicality of the adj-noun pair."}
# full_turk_counts <- read_csv(here("data/judgments_session.csv"))
# 
# full_turk_counts_plot <- full_turk_counts
# 
# full_turk_counts_plot$session2 <- cut(full_turk_counts_plot$session, 
#                    breaks=c(0, 2, 4, 6, 8, 10, 12),
#                    labels = c(14, 22, 30, 38, 42, 50))
#                    # breaks=c(0, 3, 6, 9, 12, 10, 12))

# full_expanded <- full_turk_counts[rep(row.names(full_turk_counts), full_turk_counts$n), 1:17]

token_data %>%
  mutate(typicality=mean_typ) %>%
  mutate(age = (4*session + 10)) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))
```

```{r models}
model_judgments <- read_csv(here("data/all_judgments.csv"))

tidy_turk_counts <- type_data_by_session %>%
    # select(-noun_conc, -noun_freq) %>%
    pivot_longer(cols = x1:x5, names_to = "rater", values_to = "score") %>%
  ungroup() %>%
    mutate(centered_score = score - 4,
           article = case_when(is.na(article) ~ "",
                               article %in% c("a","an") ~ "a/an",
                               T ~ "the")) %>%
  left_join(model_judgments %>% select(-turker_judgment), by = c("adj","noun"))

tidy_turk_counts_for_plots <- tidy_turk_counts %>%
  distinct(noun,adj,mean_typ,ldp_similarity,wiki_similarity)

mean_type <- lmer(centered_score ~ 1 + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_type_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                     data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()

mean_token_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                                  weights = n, data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()


mean_token <- lmer(centered_score ~ 1 + (1|noun), weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


token_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


type_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


mean_coca_token <- coca_data %>%
  filter(type == "spok") %>%
  pivot_longer(cols = c(x1:x5), names_to = "rater", values_to = "score") %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_kid_token <- kid_production %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")


```

```{r result-effects}
token_estimate <- mean_token %>% pull(estimate)
token_statistic <- mean_token %>% pull(statistic)
token_p <- mean_token %>% pull(p.value) %>% printp()

token_age_estimate <- mean_token_byage %>% slice(1) %>% pull(estimate)
token_age_statistic <- mean_token_byage %>% slice(1) %>% pull(statistic)
token_age_p <- mean_token_byage %>% slice(1) %>% pull(p.value) %>% printp()

coca_token_estimate <- mean_coca_token %>% pull(estimate)
coca_token_statistic <- mean_token %>% pull(statistic)
coca_token_p <- mean_token %>% pull(p.value) %>% printp()

token_development_estimate <-token_weight %>% filter(term == "log(age)") %>% 
  pull(estimate)
token_development_statistic <-token_weight %>% filter(term == "log(age)") %>% 
  pull(statistic)
token_development_p <-token_weight %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()

kid_token_estimate <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(estimate)
kid_token_statistic <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(statistic)
kid_token_p <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(p.value) %>% printp()
```


## Results

To estimate the relative frequency with which caregivers refer to typical vs. atypical referents, we used Turker's judgments about the atypicality of each adjective-noun pair in the data. In our analyses, we token-weighted these judgments--giving higher weight to pairs that occurred more frequently in children's inputs. However, results are qualitatively identical and all signiifcant effects remain significant without these re-weightings.

If caregivers speak informatively to convey what is atypical or surprising in relation to their own sophisticated world knowledge, we should see that caregiver description is dominated by modifiers that are sometimes true of the noun they modify. If instead child-directed speech privledges redundant information perhaps to align to young children's limited world knowledge, caregiver description should yield a distinct distribution dominated by highly typical modifiers. As can be seen in Figure \ref{fig:distribution_plot} parents' description largely focuses on features that are only sometimes true of the concept.

To confirm this effect statistically, we first centered the ratings (i.e. "about half" was coded 0), and then predicted the rating on each trial with a mixed effect model with only an intercept and a random effect of noun (\texttt{typicality $\sim$ 1 + (1|noun)}). The intercept was reliably negative, indicating that adjective tends to refer to atypical features of objects ($\beta =$ `r token_estimate`, $t =$ `r token_statistic`, $p$ `r token_p`). We then re-estimated these models seperately for each age in the corpus, and found a reliablly negative intercept for every age group (smallest effect $\beta_{14} =$ `r token_age_estimate`, $t =$ `r token_age_statistic`, $p =$ `r token_age_p`).  These data suggest that even when talking with very young children, caregiver speech is structured according to communicative pressures observed in the lab. 

<!-- Examining usage data as a function of typicality (see Figure \ref{fig:distribution_plot}), we see evidence of a positive skew (0.65). Data from every time point from 14-58 months seems to show a similar pattern (skews 0.23 - 0.82). These skews provide further evidence that the the bulk of caregiver language reflects lower-typicality adjective-noun pairs. -->

For comparison, we performed the same analyses but with typicality judgments weighted not by the frquency of each adjective-noun pair's occurrence in the Language Development Project, but instead by their frequency of occurrence in the Corpus of Contemporary American English [COCA; @davies2008]. While this estimate of adult usage is imperfect--as the adjective-nouns pairs produced by parents in our corpus may not be a representative sample of adjectives and nouns spoken by the adults in COCA, it provides a first approximation to adult usage. When we fit the same mixed-effects model to the data, we found that the intercept was reliably negative, indicating that adult-directed peech is likely also biased toward description of atypical features ($\beta =$ `r coca_token_estimate`, 
$t =$ `r coca_token_statistic`, $p$ `r coca_token_p`)

```{r compute_prototypicals}
prototypical_ratings <- type_data_by_session %>%
  mutate(typical = mean_typ >= 5)

#look for prototypicals
#  defined as anything 5 or higher, "somewhat typical" to "extremely  typical"
prototypicals <- prototypical_ratings %>%
  group_by(age, typical) %>%
  summarise(weighted_sum = sum(n), sum = n()) %>%
  group_by(age, typical) %>%
  summarise(sum = sum(sum), weighted_sum = sum(weighted_sum)) %>%
  pivot_longer(cols = c(sum, weighted_sum), names_to = "measure", values_to = "sum") %>%
  group_by(age, measure) %>%
  mutate(prop = sum / sum(sum)) %>%
  filter(typical)


typical_type_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")

typical_token_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
                      weights = n,
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")
```

```{r typical-effects}
typical_effect <- typical_token_lmer %>% filter(term == "log(age)") %>%
  pull(estimate)
typical_statistic <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(statistic)
typical_p <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()
```


Returning to caregiver speech, while descriptions at every age tended to point out atypical features as in adult-direct speech, this effect changed in strength over development. An age effect added to the previous model was reliably negative, indicating that parents of older children are relatively more likely to focus on atypical features  ($\beta =$ `r token_development_estimate`, $t =$ `r token_development_statistic`, $p =$ `r token_development_p`). In line with our hypotheses, it seems that caregivers are more likely to provide description of typical features for their young children, compared with older children. As a second test of this intuition, we defined adjectives as highly-typical if Turkers judged them to be 'often', 'almost always', or 'always' true. We predicted whether each judgment was highly typical from a mixed-effects logistic regression with a fixed effect of age (log-scaled) and a random effect of noun. Age was a highly-reliable predictor ($\beta =$ `r typical_effect`, 
$t =$ `r typical_statistic`, $p =$ `r typical_p`). While children at all ages hear more talk about what is atypically true (Figure  \ref{fig:distribution_plot}), younger children hear relatively more talk about what is typically true than older children do (Figure \ref{fig:prototypical_plot}).

```{r prototypical_plot, fig.width=3.6, fig.align = "center", num.cols.cap=1, fig.cap = "Proportion of caregiver description that is about typically-true features, as a function of age."}

prototypicals %>% 
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n typical of modified noun") +
  xlab("Child's Age (months)") +
  scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none",
        aspect.ratio = 1/1.62) 
```



### Child Speech.

Given the striking consistency in adult-to-adult speech and caregiver speech across ages, we next briefly consider what kind of information is contained in children's speech. By analyzing children's own utterances, we can determine when children come to use description in a way that looks like caregiver speech. Are children mirroring adult-like uses of description even from a young age, or are they choosing to describe more typical features of the world?

The Language Development Corpus contains 442,048 child utterances. Using the set of adjective-noun pairs for which we have judgments from our analysis of caregiver speech, we repeat our analysis on usage data for a set of `r kid_total` distinct adjective-noun pairs, `r kid_frac` of which appeared in children's productions. While preliminary, a mixed effects model predicting typicality had a highly-reliable negative intercept ($\beta =$ `r kid_token_estimate`, $t =$ `r kid_token_statistic`, $p =$ `r kid_token_p`), but adding an age term did not improve model fit. Thus, children's speech is also biased towards atypical descriptions, and this bias does not change reliably over the first 5 years.

<!-- Mirroring caregiver and adult-adult speech, children's productions show a positive skew (0.61, comapre with skewness = 0.65 seen in the adults), such that the bulk of language reflects adjective-noun pairs rated < 4 on typicality (i.e. 'sometimes', 'rarely' or 'never'). -->

## Discussion

In sum, we find robust evidence that language is used to discuss atypical, rather than typical, features of the world. Description in caregiver speech seems to largely mirror the usage patterns that we observed in adult-adult speech, suggesting that these patterns arise from general communicative pressures. Indeed, even children's own productions show a similar usage pattern, with more description of atypical features of the world as early as we can measure.

It should be noted that children's utterances come from naturalistic conversations with caregivers, and their use of atypical description may be prompted by parent-led discourse. That is, if a caregiver chooses to describe the *purpleness* of a cat in book, the child may well respond by asking about that same feature. Future analyses would need to better disentangle the extent to which children's productions are imitative of caregivers. 

<!-- their pattern of description is likely highly dependent on their caregiver's utterances.... Thus, it is possible that some of the children's use of atypical description is prompted by a parent-led discourse. -->

Interestingly, the descriptions children hear change over development, becoming increasingly focused on atypical features. The higher prevalance of typical descriptors in early development may help young learners learn what is typical; however, even at the earliest point we measured, the bulk of language input describes atypical features. 

<!-- Across adult, parent, and child language corpora, we find robust evidence that language use systematically overerpresents atypical features.  -->

This usage pattern aligns with the idea that language is used informatively in relation to background knowledge about the world. It may pose a problem, however, for young language learners with still-developing world knowledge. If language does not transparently convey the typical features of objects, and instead (perhaps misleadingly) notes the atypical ones, how might children come to learn what objects are typically like? One possibility is that information about typical features is captured in regularities across many utterances. If this is true, language may still be an important source of information about typicality as children may be able to extract more accurate typicality information by tracking second-order co-occurence.

# Extracting Structure from Language

Much information can be gleaned from language that does not seem available at first glance. From language alone, simple distributional learning models can recover enough information to perform comparably to non-native college applicants on the Test of English as a Foreign Language [@landauer1997]. Recently, @lewis2019 demonstrated that even nuanced feature information may be learnable through distributional semantics alone, without any complex inferential machinery. We take a similar approach to ask whether a distributional semantics model trained on the language children hear can capture typical feature information. 

```{r word2vec cors}
word2vec_cor_data <- tidy_turk_counts %>%
  filter(!(adj == noun)) %>%
  group_by(adj, noun, wiki_similarity, ldp_similarity) %>%
  summarise(typicality = mean(score, na.rm = T), n = sum(n)) 

ldp_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(ldp_similarity)) %>%
  as_tibble()

wiki_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()


models_cor <- wtd.cor(word2vec_cor_data %>% pull(ldp_similarity),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()

ldp_cor_estimate <- ldp_cor %>% pull(correlation)
ldp_cor_p <- ldp_cor %>% pull(p.value) %>% printp()


wiki_cor_estimate <- wiki_cor %>% pull(correlation)
wiki_cor_p <- wiki_cor %>% pull(p.value) %>% printp()

models_cor_estimate <- models_cor %>% pull(correlation)
models_cor_p <- models_cor %>% pull(p.value) %>% printp()

```

```{r, eval = FALSE}
split_half <- tidy_turk_counts %>%
  mutate(rater = as.numeric(gsub("x", "", rater))) %>%
  filter(rater %in% c(1, 2)) %>%
  group_by(rater, noun, adj) %>%
  summarise(score = mean(score, na.rm = T)) %>%
  pivot_wider(names_from = rater, values_from = score)

half_model <- lmer(`1` ~ `2` + (1|adj) + (1|noun), data = split_half)

split_half %>%
  ungroup() %>%
  mutate(prediction = predict(half_model)) %>%
  summarise(cor = cor(`1`, prediction))

data <- tidy_turk_counts_for_plots %>% 
       left_join(tidy_turk_counts %>% filter(rater == "x1"))

cor(data$score, data$wiki_similarity, use = "pairwise")

wiki_model <- lm(score ~ wiki_similarity ,
     data = data) 

tidy_turk_counts_for_plots %>%
  filter(!is.na(wiki_similarity)) %>%
  mutate(prediction = predict(wiki_model)) %>%
  summarise(cor = cor(prediction, turker_judgment))

```

```{r word2vec-pairs}
pairs <- tidy_turk_counts %>%
  group_by(noun, adj) %>%
  summarise(typicality = mean(score, na.rm = TRUE)) %>%
  group_by(noun) %>%
  mutate(max_typ = max(typicality),
         min_typ = min(typicality)) %>%
  distinct(noun, min_typ, max_typ) %>%
  filter(min_typ != max_typ, max_typ >= 5, min_typ <= 3) 

high_low_pairs <- tidy_turk_counts_for_plots %>%
  filter(noun %in% pairs$noun) %>%
  left_join(pairs, by = c("noun")) %>%
  filter(mean_typ == min_typ | mean_typ == max_typ) %>%
  select(adj, noun, mean_typ) %>%
  group_by(noun) %>%
  arrange(noun, desc(mean_typ)) %>%
  slice(1, n()) %>%
  mutate(typicality = c("high", "low"),
         typicality = factor(typicality, levels = c("low", "high"))) %>%
  left_join(tidy_turk_counts_for_plots)

correct_orders <- high_low_pairs %>%
  pivot_longer(cols = c(ldp_similarity, wiki_similarity), 
               names_to = "measure", values_to = "similarity") %>%
  select(-adj, -mean_typ) %>%
  pivot_wider(names_from = "typicality", values_from = "similarity") %>%
  mutate(correct = high - low > 0) %>%
  filter(!is.na(correct)) %>%
  group_by(measure) %>%
  summarise(correct = sum(correct), total = n())

ldp_correct <- correct_orders %>%
  filter(measure == "ldp_similarity") %>%
  pull(correct)

wiki_correct <- correct_orders %>%
  filter(measure == "wiki_similarity") %>%
  pull(correct)

pairs_total <- correct_orders %>% pull(total) %>% first()

ldp_binom <- binom.test(ldp_correct, pairs_total)$p.value %>%
  printp()
wiki_binom <- binom.test(wiki_correct, pairs_total)$p.value %>%
  printp()
```

```{r halfs, fig.width=3.4, fig.height = 3.4, fig.align = "center", num.cols.cap=1, fig.cap = 'Plots of nouns for which there was at least one atypical adjective (rated at most "sometimes"), and at least one typical adjective ("rated at least often"). The black dotted line shows average human typicality ratings (scaled) for these items. The blue line shows how well our models do at capturing this trend, with grey lines representing individual pairs.'}
halves_data <- high_low_pairs %>%
  pivot_longer(cols = c(mean_typ, wiki_similarity, ldp_similarity),
              names_to = "measure", values_to = "score") %>%
  mutate(measure = factor(measure, 
                          levels = c("mean_typ", "ldp_similarity", 
                                     "wiki_similarity"), 
                          labels = c("Human", "LDP Word2Vec", 
                                     "Wiki Word2Vec"))) %>%
  mutate(score = if_else(measure=="Human", score/7, score)) %>%
  filter(! noun == "orange") %>%
    filter(! adj == "orange") 

human <- halves_data %>%
  filter(measure =="Human") %>%
  group_by(typicality) %>% 
  summarise(mean=mean(score))

means <- halves_data %>%
    group_by(measure, typicality) %>% 
    summarise(mean=mean(score, na.rm=T)) %>%
    filter(measure != "Human")


halves_data %>%
  filter(measure != "Human") %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_grid(measure ~ .) +
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  geom_line(data=human, aes(x=typicality, y=mean, group=NA), color="black", linetype="dashed") +
  geom_point(data=human, aes(x=typicality, y=mean, group=NA), color="black") +
  geom_line(data=means, aes(x=typicality, y=mean, group=NA), color="steelblue", size=.8) +
  geom_point(data=means, aes(x=typicality, y=mean, group=NA), color="steelblue") +
  labs(x = "Binned Human-Rated Typicality", y = "Cosine Similarity") +
  theme_few() +
  coord_cartesian(ylim=c(-0.17,1))
```


```{r pairs_tab, results="asis", tab.env = "table"}
pair_tab <- high_low_pairs %>%
  group_by(noun) %>%
  summarise(diff = first(wiki_similarity) - last(wiki_similarity)) %>%
  filter(diff < 0) %>%
  left_join(high_low_pairs %>% select(noun, adj)) %>%
  group_by(noun) %>%
  mutate(typicality = c("high", "low")) %>%
  pivot_wider(names_from = "typicality", values_from = "adj") %>%
  select(diff, noun, high, low) %>%
  arrange(diff) %>%
  ungroup() %>%
  slice(1:10) %>%
  select(-diff) %>%
  xtable(caption = "The ten cases in which word2vec similarities are worst at predicting human typicality judgments. For each of these cases, the model judges the low-typicality adjective to be more similar to the noun than the high-typicality adjective.",
         label = "tab:pairs_tab")

print(pair_tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      include.rownames = FALSE)
```

## Method

To test this possibility, we trained word2vec, a model that predicts words using their contexts, on the same corpus of child-directed speech used in our first set of analyses. Our model is a continuous-bag-of-words word2vec model trained using the package gensim [@rehurek2010]. If the model captures information about the typical features of objects, we should see that the model's word pair similarities are correlated with the typicality ratings we elicited from human raters. For a second comparison, we also used an off-the-shelf implementation of word2vec trained on Wikipedia [@mikolov2018]. While the Language Development Project corpus likely underestimates the amount of structure in children's linguistic input, Wikipedia likely overestimates it.

## Results

We find that similarities in the model trained on the Language Development Project corpus have near zero correlation with human adjective–noun typicality ratings ($r =$ `r ldp_cor_estimate`, $p =$ `r ldp_cor_p`). This is in spite of better correlations with large sets of human similarity judgments between different kinds of word pairs (correlation with  wordsim353, 0.37; correlation with simlex, 0.15). This suggests that statistical patterns in child-directed speech are likely insufficient to encode information about the typical features of objects, despite encoding at least some information about word meaning more broadly. 

However, the corpus on which we trained this model was small; perhaps our model did not get enough language to draw out the patterns that would reflect the typical features of objects. To test this possibility, we asked whether word vectors trained on a much larger corpus—English Wikipedia—strongly correlate with typicality ratings. This model's similarities were signifcantly correlated with human judgments, although the strength of the correlation was still fairly weak ($r =$ `r wiki_cor_estimate`, $p$ `r wiki_cor_p`). Interestingly, similarities from the two models wrere more highly correlated than either model's similarity and human judgments ($r =$ `r models_cor_estimate`, $p$ `r models_cor_p`). This suggests that these models are picking up on some systematic associations between nouns and adjectives, but not typicality inofrmation ones. 

One possibile confound in these analyses is that the similarity judgments produced by our models reflect many dimensions of similarity, but human judgments reflect only typicality. To accomodate this, we performed a second analysis where we considered only the subset of `r pairs_total` nouns that had both a typical (rated as at least "often") and an atypical (rated as at most "sometimes") adjective. We then asked whether the models rated the typical adjective as more similar to the noun it modified than the atypical noun. The LDP model correctly classified `r ldp_correct` out of `r pairs_total` (`r ldp_correct / pairs_total`), which was not better than chance ($p =$ `r ldp_binom`). The Wikipedia model correctly classified `r wiki_correct` out of `r pairs_total` (`r wiki_correct / pairs_total`), which was better than chance according to a binomial test, but still fairly poor perfomance ($p =$ `r wiki_binom`). Fig \ref{fig:halfs} shows the ratings from Turkers and the two models for the `r pairs_total` nouns. Table \ref{tab:pairs_tab} shows the 10 adjective-noun pairs where word2vec trained on Wikipedia produce judgments that are farthest away from the human judgments for these nouns (LDP was similar).  **fix up the discussion here**

# General Discussion

Language provides children a rich source of information about the world. However, this information is not always transparently available: becuase language is used to comment on the atypical and surprising, it does not perfectly mirror the world. Among adult conversational partners whose world knowledge is well-aligned, this characteristic of language allows people to converse informatively and not redundantly. But between a child and caregiver whose world knowledge is asymmetric, this pressure competes with other demands: what is minimally informative to an adult may be misleading to a child. Our results show that this pressure structures language to create a peculiar learning environment; one in which caregivers predominantly point out the atypical features of things. 

How, then, do children learn about the typical features of things from such an environment? While younger children may gain an important foothold from hearing more description of typical features, they still face language dominated by atypical description. When we looked at more nuanced ways of extracting information from language (which may or may not be available to the developing learner), we found that models of distributional semantics capture little typical feature information.

Of course, one source of information that may simplify this problem is perceptual information from the world itself. In many cases, perceptual information may swamp information from language; children likely see enough orange carrots in the world to outweigh hearing "purple carrot.” It remains unclear, however, how children learn about categories for which they have scarcer evidence. Indeed, language information likely swamps perceptual information for many other categories, such as abstract concepts or those that cannot be learned about by direct experience. Given that the present work is limited to concrete concepts, we can only speculate about the information caregivers provide about abstract concepts. But, if abstract concepts pattern similarly to concrete objects, children are in a particularly difficult bind. Though perceptual information is undoubtedly useful in learning about the typical features of things, it remains to be explained how children learn what is typical when this perceptual information is scant, irrelevant, or incomplete.

Another possibility is that children expect language to be used informatively at a young age. Under this hypothesis, their language environment is not misleading at all. If young children expect adjectives to mark atypical features, they can use description and the lack thereof to learn more about the world around them. Lab studies find that children expect interlocutors to be informative in relation to their prior knowledge by the age of 2 [@akhtar1996] and that 5–6-year-old children are roughly informative with respect to their interlocutor’s perspective in a referential communication task [@nadig2002]. The idea that young children understand the informative purpose of language is consistent with our finding that even young children also largely choose to describe atypical features. Though this effect can be explained by simpler means such as salience or mimicry, it suggests that caregivers and children may be usefully aligned in the aspects of the world they choose to talk about.

<!-- More naturalistic studies find that children at the one-word stage selectively imitate words that convey new information—that is, words that are informative with respect to what is obvious or assumed in the environment—when describing events (Greenfield & Zukow, 1978). -->

Whether adult-directed, child-directed, or a child's own speech, language is used with remarkable consistency: people talk about the atypical. Though parents might reasonably be broadly over-informative in order to teach their children about the world, this is not the case. This presents a potential puzzle for young learners who have limited world knowledge and limited pragmatic inferential abilities. Perceptual information and nascent pragmatic abilities may help fill in the gaps, but much remains to be explored to link these explanations to actual learning. Communication pressures are pervasive forces structuring the language children hear, and future work must disentangle whether children capitalize on them or are misled by them in learning about the world.

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Data and analysis code will be made available through GitHub after de-anonymization. \ }}

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
